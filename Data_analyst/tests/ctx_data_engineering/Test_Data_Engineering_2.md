# Test de Pr√©paration: Data Engineering Avanc√©

## Informations
- **Dur√©e estim√©e:** 40 minutes
- **Nombre de questions:** 25
- **Niveau:** Avanc√©
- **Th√®mes:** ETL/ELT, Qualit√© des donn√©es, Orchestration, Formats, Optimisation

---

## Section 1: ETL vs ELT (5 questions)

### Question 1
Quelle est la principale diff√©rence entre ETL et ELT?

A) ETL est plus rapide que ELT
B) Dans ETL, la transformation se fait avant le chargement; dans ELT, apr√®s
C) ETL est pour les petits volumes, ELT pour les grands
D) Il n'y a pas de diff√©rence

<details>
<summary>R√©ponse</summary>

**B) Dans ETL, la transformation se fait avant le chargement; dans ELT, apr√®s**

| Approche | Flux | Avantage |
|----------|------|----------|
| **ETL** | Extract ‚Üí **Transform** ‚Üí Load | Donn√©es propres √† l'arriv√©e |
| **ELT** | Extract ‚Üí Load ‚Üí **Transform** | Exploite la puissance du DWH cloud |

ETL: Classique, on-premise, transformation sur serveur ETL
ELT: Moderne, cloud, transformation dans le data warehouse (BigQuery, Snowflake)
</details>

---

### Question 2
Dans quel sc√©nario ELT est-il pr√©f√©rable √† ETL?

A) Quand les donn√©es sont d√©j√† propres
B) Quand on utilise un data warehouse cloud avec grande capacit√© de calcul
C) Quand on a tr√®s peu de donn√©es
D) Quand on n'a pas besoin de transformer les donn√©es

<details>
<summary>R√©ponse</summary>

**B) Quand on utilise un data warehouse cloud avec grande capacit√© de calcul**

ELT est pr√©f√©rable quand:
- Data warehouse cloud puissant (BigQuery, Snowflake, Redshift)
- Grands volumes de donn√©es
- Besoin de flexibilit√© (sch√©ma-on-read)
- Transformation avec SQL (dbt)

ETL reste pr√©f√©rable quand:
- Contraintes de conformit√© (donn√©es sensibles)
- Logique de transformation complexe en Python/Java
- Infrastructure on-premise
</details>

---

### Question 3
Quel outil est sp√©cialis√© dans la transformation de donn√©es dans un entrep√¥t (paradigme ELT)?

A) Apache Spark
B) dbt (data build tool)
C) Talend
D) Pentaho

<details>
<summary>R√©ponse</summary>

**B) dbt (data build tool)**

dbt (data build tool):
- Transformations SQL dans le data warehouse
- Versionning avec Git
- Tests de donn√©es int√©gr√©s
- Documentation automatique
- Lineage des donn√©es

```sql
-- models/staging/stg_clients.sql
SELECT
    id AS client_id,
    TRIM(nom) AS nom,
    DATE(created_at) AS date_creation
FROM {{ source('raw', 'clients') }}
WHERE deleted_at IS NULL
```

dbt compile et ex√©cute les transformations directement dans le DWH.
</details>

---

### Question 4
Qu'est-ce que le concept de "staging area" dans un pipeline de donn√©es?

A) La zone de production
B) Une zone interm√©diaire o√π les donn√©es brutes sont stock√©es avant transformation
C) La base de donn√©es de backup
D) L'interface utilisateur

<details>
<summary>R√©ponse</summary>

**B) Une zone interm√©diaire o√π les donn√©es brutes sont stock√©es avant transformation**

Architecture en couches (Medallion):

```
Sources ‚Üí [Bronze/Raw] ‚Üí [Silver/Staging] ‚Üí [Gold/Mart] ‚Üí Rapports
              ‚Üì               ‚Üì                ‚Üì
           Donn√©es         Donn√©es         Donn√©es
           brutes          nettoy√©es       agr√©g√©es
```

La staging area permet:
- Isolation des donn√©es brutes
- Rejeu des transformations si erreur
- Tra√ßabilit√© et audit
- S√©paration des responsabilit√©s
</details>

---

### Question 5
Comment g√©rer un pipeline ETL qui doit s'ex√©cuter apr√®s la fin d'un autre pipeline?

A) Ex√©cuter manuellement
B) Utiliser un orchestrateur avec d√©pendances (Airflow, Prefect)
C) Mettre un timer fixe
D) Dupliquer le code

<details>
<summary>R√©ponse</summary>

**B) Utiliser un orchestrateur avec d√©pendances (Airflow, Prefect)**

Exemple Airflow:
```python
from airflow import DAG
from airflow.operators.python import PythonOperator

with DAG('pipeline_bancaire', schedule_interval='@daily') as dag:
    
    extract_task = PythonOperator(
        task_id='extract_transactions',
        python_callable=extract_transactions
    )
    
    transform_task = PythonOperator(
        task_id='transform_transactions',
        python_callable=transform_transactions
    )
    
    load_task = PythonOperator(
        task_id='load_to_dwh',
        python_callable=load_to_dwh
    )
    
    # D√©finir les d√©pendances
    extract_task >> transform_task >> load_task
```
</details>

---

## Section 2: Qualit√© des Donn√©es (5 questions)

### Question 6
Quelles sont les dimensions principales de la qualit√© des donn√©es?

A) Taille, Couleur, Format
B) Exactitude, Compl√©tude, Coh√©rence, Actualit√©
C) Vitesse, Volume, Vari√©t√©
D) Input, Process, Output

<details>
<summary>R√©ponse</summary>

**B) Exactitude, Compl√©tude, Coh√©rence, Actualit√©**

Dimensions de la qualit√© des donn√©es (ACCA):

| Dimension | Question | Exemple bancaire |
|-----------|----------|------------------|
| **Exactitude** | Les donn√©es sont-elles correctes? | Solde = vraie valeur |
| **Compl√©tude** | Manque-t-il des donn√©es? | % valeurs manquantes |
| **Coh√©rence** | Les donn√©es sont-elles logiques? | Solde ‚â• 0 pour √©pargne |
| **Actualit√©** | Les donn√©es sont-elles r√©centes? | Date derni√®re MAJ |

Autres dimensions: Unicit√©, Validit√©, Conformit√©
</details>

---

### Question 7
Comment impl√©menter une validation de r√®gle m√©tier "Le solde ne peut pas √™tre n√©gatif pour un compte √©pargne"?

A) Ne rien faire
B) Cr√©er un test de qualit√© avec assertion
C) Supprimer les lignes concern√©es
D) Modifier manuellement les donn√©es

<details>
<summary>R√©ponse</summary>

**B) Cr√©er un test de qualit√© avec assertion**

```python
def validate_solde_epargne(df):
    """
    Valide que les comptes √©pargne n'ont pas de solde n√©gatif
    """
    violations = df[(df['type_compte'] == 'EPARGNE') & (df['solde'] < 0)]
    
    if len(violations) > 0:
        # Logger les violations
        print(f"‚ö†Ô∏è {len(violations)} comptes √©pargne avec solde n√©gatif")
        print(violations[['compte_id', 'solde']])
        
        # Option: lever une exception
        raise ValueError("Violation de r√®gle m√©tier: solde n√©gatif sur √©pargne")
    
    return True

# Avec Great Expectations
expect_column_values_to_be_between(
    column="solde",
    min_value=0,
    row_condition='type_compte=="EPARGNE"'
)
```
</details>

---

### Question 8
Quel outil open-source est sp√©cialis√© dans les tests de qualit√© des donn√©es?

A) Pandas
B) Great Expectations
C) Matplotlib
D) Flask

<details>
<summary>R√©ponse</summary>

**B) Great Expectations**

Great Expectations permet de:
- D√©finir des "expectations" (r√®gles de qualit√©)
- Valider les donn√©es automatiquement
- G√©n√©rer des rapports de qualit√©
- Int√©grer dans les pipelines

```python
import great_expectations as gx

# Cr√©er un contexte
context = gx.get_context()

# D√©finir des expectations
validator.expect_column_values_to_not_be_null("client_id")
validator.expect_column_values_to_be_between("age", 18, 120)
validator.expect_column_values_to_be_in_set(
    "segment", ["RETAIL", "PREMIUM", "PRIVATE"]
)

# Valider
results = validator.validate()
```
</details>

---

### Question 9
Qu'est-ce que le "data profiling"?

A) La cr√©ation de profils utilisateurs
B) L'analyse exploratoire automatique de la structure et qualit√© des donn√©es
C) La s√©curisation des donn√©es
D) La compression des donn√©es

<details>
<summary>R√©ponse</summary>

**B) L'analyse exploratoire automatique de la structure et qualit√© des donn√©es**

Le data profiling g√©n√®re automatiquement:
- Types de donn√©es d√©tect√©s
- Statistiques (min, max, moyenne, distribution)
- Taux de valeurs manquantes
- Cardinalit√© (valeurs uniques)
- Patterns d√©tect√©s

```python
# Avec pandas-profiling (ydata-profiling)
from ydata_profiling import ProfileReport

profile = ProfileReport(df, title="Profil des Donn√©es Clients")
profile.to_file("rapport_profil.html")

# Informations g√©n√©r√©es:
# - Aper√ßu des colonnes
# - Alertes de qualit√©
# - Corr√©lations
# - Valeurs manquantes
# - √âchantillons
```
</details>

---

### Question 10
Comment d√©tecter les doublons dans un dataset client?

A) Compter le nombre de lignes
B) Utiliser duplicated() ou group by sur les cl√©s d'identification
C) Trier les donn√©es
D) Calculer la moyenne

<details>
<summary>R√©ponse</summary>

**B) Utiliser duplicated() ou group by sur les cl√©s d'identification**

```python
# M√©thode 1: duplicated()
doublons = df[df.duplicated(subset=['nom', 'date_naissance', 'telephone'], keep=False)]
print(f"Nombre de doublons: {len(doublons)}")

# M√©thode 2: group by et count
doublons_group = df.groupby(['nom', 'date_naissance', 'telephone']).size()
doublons_group = doublons_group[doublons_group > 1]

# M√©thode 3: Fuzzy matching pour doublons approximatifs
from fuzzywuzzy import fuzz
# Comparer les noms avec tol√©rance aux fautes de frappe
```

Les doublons peuvent causer:
- Sur√©valuation du nombre de clients
- Erreurs dans les agr√©gations
- Probl√®mes de conformit√© (KYC)
</details>

---

## Section 3: Formats et Stockage (5 questions)

### Question 11
Quel format de fichier est le plus efficace pour le stockage analytique de grandes tables?

A) CSV
B) Parquet
C) JSON
D) Excel

<details>
<summary>R√©ponse</summary>

**B) Parquet**

Comparaison des formats:

| Format | Type | Compression | Lecture colonnes | Usage |
|--------|------|-------------|------------------|-------|
| CSV | Texte | Faible | Non | √âchange simple |
| JSON | Texte | Faible | Non | APIs, semi-structur√© |
| Parquet | Binaire | Excellente | Oui | Analytics, Big Data |
| Avro | Binaire | Bonne | Non | Streaming |

Avantages Parquet:
- Stockage en colonnes (lecture s√©lective)
- Compression efficace (jusqu'√† 90%)
- Sch√©ma int√©gr√©
- Compatible Spark, Pandas, BigQuery
</details>

---

### Question 12
Qu'est-ce que le partitionnement dans un data lake?

A) Diviser le stockage entre plusieurs serveurs
B) Organiser les fichiers en sous-dossiers bas√©s sur une colonne (ex: date)
C) Compresser les donn√©es
D) Crypter les fichiers

<details>
<summary>R√©ponse</summary>

**B) Organiser les fichiers en sous-dossiers bas√©s sur une colonne (ex: date)**

Structure partitionn√©e:
```
data/
‚îú‚îÄ‚îÄ transactions/
‚îÇ   ‚îú‚îÄ‚îÄ year=2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=02/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/
‚îÇ       ‚îî‚îÄ‚îÄ ...
```

Avantages:
- Lecture s√©lective (partition pruning)
- Meilleure performance des requ√™tes filtr√©es
- Gestion facilit√©e (archivage par p√©riode)

```python
# √âcriture partitionn√©e avec Pandas
df.to_parquet(
    'data/transactions',
    partition_cols=['year', 'month']
)
```
</details>

---

### Question 13
Quelle strat√©gie de mise √† jour utiliser pour un historique complet des soldes clients?

A) √âcraser les donn√©es chaque jour
B) Slowly Changing Dimension (SCD) Type 2
C) Supprimer et recr√©er
D) Ignorer les changements

<details>
<summary>R√©ponse</summary>

**B) Slowly Changing Dimension (SCD) Type 2**

SCD Type 2 conserve l'historique complet:

| client_id | solde | date_debut | date_fin | actuel |
|-----------|-------|------------|----------|--------|
| 001 | 50000 | 2024-01-01 | 2024-03-15 | 0 |
| 001 | 75000 | 2024-03-16 | 2024-06-30 | 0 |
| 001 | 100000 | 2024-07-01 | 9999-12-31 | 1 |

Types SCD:
- **Type 1:** √âcraser (pas d'historique)
- **Type 2:** Nouvelle ligne avec dates (historique complet)
- **Type 3:** Nouvelle colonne (historique limit√©)

Bancaire: SCD2 essentiel pour audit et conformit√©.
</details>

---

### Question 14
Qu'est-ce que le CDC (Change Data Capture)?

A) Un format de fichier
B) Une technique pour capturer uniquement les changements dans les donn√©es sources
C) Un type de base de donn√©es
D) Un outil de visualisation

<details>
<summary>R√©ponse</summary>

**B) Une technique pour capturer uniquement les changements dans les donn√©es sources**

CDC capture:
- Insertions (nouvelles lignes)
- Mises √† jour (modifications)
- Suppressions

Avantages:
- R√©duction du volume transf√©r√©
- Latence plus faible
- Moins de charge sur la source

Outils: Debezium, AWS DMS, Fivetran

```
Source DB ‚Üí [CDC] ‚Üí Message Queue ‚Üí [Consumer] ‚Üí Data Lake
              ‚Üì
         Capture des
         changements
         en temps r√©el
```
</details>

---

### Question 15
Comment optimiser une requ√™te BigQuery qui scanne trop de donn√©es?

A) Ajouter plus de colonnes dans le SELECT
B) Utiliser le partitionnement et le clustering
C) Supprimer les index
D) Augmenter le timeout

<details>
<summary>R√©ponse</summary>

**B) Utiliser le partitionnement et le clustering**

Optimisations BigQuery:

```sql
-- Cr√©er une table partitionn√©e et cluster√©e
CREATE TABLE `projet.dataset.transactions`
PARTITION BY DATE(date_transaction)
CLUSTER BY agence_id, client_id
AS SELECT * FROM `projet.dataset.transactions_raw`;

-- Requ√™te optimis√©e
SELECT client_id, SUM(montant)
FROM `projet.dataset.transactions`
WHERE date_transaction BETWEEN '2024-01-01' AND '2024-01-31'
  AND agence_id = 'AG001'
GROUP BY client_id;
-- BigQuery ne scanne que la partition de janvier et le cluster AG001
```

Autres optimisations:
- SELECT uniquement les colonnes n√©cessaires
- √âviter SELECT *
- Utiliser les vues mat√©rialis√©es
</details>

---

## Section 4: Orchestration et Monitoring (5 questions)

### Question 16
Quel est le r√¥le d'Apache Airflow dans un pipeline de donn√©es?

A) Stockage des donn√©es
B) Orchestration et scheduling des t√¢ches
C) Visualisation des donn√©es
D) Machine Learning

<details>
<summary>R√©ponse</summary>

**B) Orchestration et scheduling des t√¢ches**

Apache Airflow:
- D√©finit des DAGs (Directed Acyclic Graphs)
- Planifie l'ex√©cution (cron-like)
- G√®re les d√©pendances entre t√¢ches
- Monitore les ex√©cutions
- G√®re les reprises apr√®s √©chec

```python
from airflow import DAG
from datetime import datetime

with DAG(
    'etl_bancaire_quotidien',
    schedule_interval='0 6 * * *',  # Tous les jours √† 6h
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    
    t1 = extract_task()
    t2 = transform_task()
    t3 = load_task()
    t4 = notify_task()
    
    t1 >> t2 >> t3 >> t4
```
</details>

---

### Question 17
Qu'est-ce qu'un "backfill" dans le contexte des pipelines de donn√©es?

A) Remplir des donn√©es manquantes dans le pass√©
B) Ex√©cuter un pipeline pour des dates historiques
C) Sauvegarder les donn√©es
D) Supprimer les anciennes donn√©es

<details>
<summary>R√©ponse</summary>

**B) Ex√©cuter un pipeline pour des dates historiques**

Backfill = Rejouer un pipeline pour des p√©riodes pass√©es

Sc√©narios:
- Nouveau pipeline √† appliquer sur l'historique
- Correction d'une erreur pass√©e
- Ajout d'une nouvelle m√©trique √† calculer r√©troactivement

```bash
# Airflow backfill
airflow dags backfill etl_bancaire \
    --start-date 2024-01-01 \
    --end-date 2024-06-30
```

‚ö†Ô∏è Attention: S'assurer que le backfill n'√©crase pas des donn√©es valides
</details>

---

### Question 18
Comment alerter l'√©quipe si un pipeline √©choue?

A) V√©rifier manuellement chaque matin
B) Configurer des callbacks d'√©chec avec notifications (email, Slack)
C) Ignorer les √©checs
D) Red√©marrer automatiquement sans notification

<details>
<summary>R√©ponse</summary>

**B) Configurer des callbacks d'√©chec avec notifications (email, Slack)**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.email import send_email

def failure_callback(context):
    """Appel√© quand une t√¢che √©choue"""
    task_instance = context['task_instance']
    
    # Envoyer email
    send_email(
        to=['data-team@unibank.ht'],
        subject=f"Pipeline {task_instance.dag_id} FAILED",
        html_content=f"T√¢che {task_instance.task_id} a √©chou√©"
    )
    
    # Ou notification Slack
    slack_notification(f"üö® Pipeline √©chou√©: {task_instance.dag_id}")

with DAG('etl_bancaire',
         default_args={'on_failure_callback': failure_callback}
        ) as dag:
    # ...
```
</details>

---

### Question 19
Qu'est-ce que l'idempotence dans un pipeline de donn√©es?

A) Un pipeline qui s'ex√©cute une seule fois
B) Un pipeline qui produit le m√™me r√©sultat peu importe le nombre d'ex√©cutions
C) Un pipeline tr√®s rapide
D) Un pipeline sans erreurs

<details>
<summary>R√©ponse</summary>

**B) Un pipeline qui produit le m√™me r√©sultat peu importe le nombre d'ex√©cutions**

Pipeline idempotent:
- Ex√©cut√© 1 fois ‚Üí R√©sultat A
- Ex√©cut√© 2 fois ‚Üí R√©sultat A (identique)
- Ex√©cut√© N fois ‚Üí R√©sultat A (toujours identique)

Comment garantir l'idempotence:
```python
# Mauvais (non idempotent): INSERT √† chaque ex√©cution
INSERT INTO table VALUES (...)  # Cr√©e des doublons!

# Bon (idempotent): DELETE + INSERT ou MERGE
DELETE FROM table WHERE date = '2024-01-15';
INSERT INTO table SELECT * FROM staging WHERE date = '2024-01-15';

# Ou avec MERGE (upsert)
MERGE INTO target USING source
ON target.id = source.id
WHEN MATCHED THEN UPDATE ...
WHEN NOT MATCHED THEN INSERT ...
```
</details>

---

### Question 20
Quelle m√©trique surveiller pour d√©tecter une d√©gradation du pipeline?

A) La couleur des logs
B) Le temps d'ex√©cution, le volume trait√©, le taux d'erreur
C) Le nom des t√¢ches
D) La version du logiciel

<details>
<summary>R√©ponse</summary>

**B) Le temps d'ex√©cution, le volume trait√©, le taux d'erreur**

M√©triques de monitoring:

| M√©trique | Alerte si | Action |
|----------|-----------|--------|
| Temps d'ex√©cution | +50% vs moyenne | Investiguer lenteur |
| Volume trait√© | -20% vs attendu | V√©rifier source |
| Taux d'erreur | > 1% | V√©rifier qualit√© |
| Latence des donn√©es | > SLA | Optimiser pipeline |
| Utilisation ressources | > 80% | Scaler infrastructure |

```python
# Exemple de validation de volume
rows_processed = len(df)
expected_rows = get_expected_rows(date)

if rows_processed < expected_rows * 0.8:
    raise ValueError(f"Volume anormal: {rows_processed} vs {expected_rows} attendus")
```
</details>

---

## Section 5: S√©curit√© et Gouvernance (5 questions)

### Question 21
Comment prot√©ger les donn√©es sensibles (NIF, num√©ro de compte) dans un pipeline?

A) Les ignorer
B) Appliquer le masquage ou la tokenisation
C) Les stocker en texte clair
D) Les supprimer compl√®tement

<details>
<summary>R√©ponse</summary>

**B) Appliquer le masquage ou la tokenisation**

Techniques de protection:

| Technique | Description | Exemple |
|-----------|-------------|---------|
| Masquage | Cacher partiellement | 1234-****-****-5678 |
| Tokenisation | Remplacer par token | ACC_TOKEN_X7Y9 |
| Hachage | Transformation irr√©versible | SHA256(NIF) |
| Chiffrement | R√©versible avec cl√© | AES(donn√©es, cl√©) |

```python
import hashlib

def masquer_compte(numero):
    """Masque le num√©ro de compte"""
    return numero[:4] + '****' + numero[-4:]

def tokeniser(valeur_sensible, secret):
    """Cr√©e un token irr√©versible"""
    return hashlib.sha256((valeur_sensible + secret).encode()).hexdigest()[:16]

# Application
df['compte_masque'] = df['numero_compte'].apply(masquer_compte)
```
</details>

---

### Question 22
Qu'est-ce que le data lineage?

A) La g√©n√©alogie des employ√©s
B) Le tra√ßage de l'origine et des transformations des donn√©es
C) Le tri des donn√©es
D) La suppression des donn√©es anciennes

<details>
<summary>R√©ponse</summary>

**B) Le tra√ßage de l'origine et des transformations des donn√©es**

Data Lineage r√©pond √†:
- D'o√π viennent ces donn√©es? (origine)
- Quelles transformations ont √©t√© appliqu√©es?
- Qui a acc√©d√© aux donn√©es?
- Quels rapports utilisent ces donn√©es?

```
[Source: Core Banking] ‚Üí [Staging] ‚Üí [Transform] ‚Üí [Data Mart] ‚Üí [Rapport]
        ‚Üì                    ‚Üì            ‚Üì            ‚Üì
     Raw tables          Nettoyage    Agr√©gation    KPIs
```

Outils: Apache Atlas, DataHub, dbt (lineage automatique)

Importance bancaire: Conformit√© r√©glementaire, audit, impact analysis
</details>

---

### Question 23
Comment g√©rer les droits d'acc√®s aux donn√©es dans un data warehouse?

A) Donner acc√®s √† tout le monde
B) Impl√©menter le RBAC (Role-Based Access Control)
C) Chiffrer tout sans donner acc√®s
D) Utiliser un seul compte partag√©

<details>
<summary>R√©ponse</summary>

**B) Impl√©menter le RBAC (Role-Based Access Control)**

```sql
-- BigQuery exemple
-- Cr√©er des r√¥les
CREATE ROLE data_analyst;
CREATE ROLE data_scientist;
CREATE ROLE data_admin;

-- Attribuer des permissions
GRANT SELECT ON DATASET reporting TO ROLE data_analyst;
GRANT SELECT, INSERT ON ALL TABLES TO ROLE data_scientist;
GRANT ALL PRIVILEGES TO ROLE data_admin;

-- Masquer des colonnes sensibles
CREATE VIEW clients_safe AS
SELECT 
    client_id,
    segment,
    ville,
    '***' AS nif_masque  -- NIF masqu√© pour analysts
FROM clients;

GRANT SELECT ON clients_safe TO ROLE data_analyst;
```

Principe du moindre privil√®ge: Donner uniquement les acc√®s n√©cessaires.
</details>

---

### Question 24
Qu'est-ce que la r√©tention des donn√©es et pourquoi est-ce important?

A) La vitesse de traitement
B) La dur√©e de conservation des donn√©es avant archivage/suppression
C) La taille des fichiers
D) Le format de stockage

<details>
<summary>R√©ponse</summary>

**B) La dur√©e de conservation des donn√©es avant archivage/suppression**

Politique de r√©tention bancaire (exemple):

| Type de donn√©es | R√©tention | Raison |
|-----------------|-----------|--------|
| Transactions | 10 ans | R√©glementation BRH |
| Logs d'acc√®s | 5 ans | Audit s√©curit√© |
| Donn√©es marketing | 3 ans | RGPD / Consentement |
| Donn√©es temporaires | 30 jours | Nettoyage |

```python
# Impl√©mentation automatique
from datetime import datetime, timedelta

def appliquer_retention(df, colonne_date, jours_retention):
    """Supprime les donn√©es au-del√† de la r√©tention"""
    date_limite = datetime.now() - timedelta(days=jours_retention)
    return df[df[colonne_date] >= date_limite]

# Archiver avant suppression
df_archive = df[df['date'] < date_limite]
df_archive.to_parquet(f'archive/{year}/data.parquet')
```
</details>

---

### Question 25
Comment documenter un pipeline de donn√©es pour faciliter la maintenance?

A) Ne pas documenter, le code est suffisant
B) Cr√©er un README, des commentaires, un catalogue de donn√©es
C) Documenter uniquement les erreurs
D) Utiliser des noms de variables al√©atoires

<details>
<summary>R√©ponse</summary>

**B) Cr√©er un README, des commentaires, un catalogue de donn√©es**

√âl√©ments de documentation:

1. **README du pipeline:**
```markdown
# Pipeline ETL Transactions
## Description
Charge les transactions quotidiennes vers le data warehouse.

## Schedule
Quotidien √† 6h00

## D√©pendances
- Source: Core Banking (Oracle)
- Destination: BigQuery

## Contact
data-team@unibank.ht
```

2. **Catalogue de donn√©es:**
```yaml
# schema.yml (dbt)
models:
  - name: transactions
    description: "Transactions clients nettoy√©es et enrichies"
    columns:
      - name: transaction_id
        description: "Identifiant unique de la transaction"
        tests: [unique, not_null]
      - name: montant
        description: "Montant en HTG"
```

3. **Commentaires dans le code:**
```python
def transform_transactions(df):
    """
    Transforme les transactions brutes.
    
    √âtapes:
    1. Suppression des doublons
    2. Conversion des montants
    3. Enrichissement avec r√©f√©rentiels
    
    Args:
        df: DataFrame des transactions brutes
    Returns:
        DataFrame transform√©
    """
```
</details>

---

## R√©sum√© et Mn√©motechniques

### "ACCA" - Qualit√© des Donn√©es
```
A - Accuracy (Exactitude): Donn√©es correctes?
C - Completeness (Compl√©tude): Donn√©es pr√©sentes?
C - Consistency (Coh√©rence): Donn√©es logiques?
A - Actuality (Actualit√©): Donn√©es r√©centes?
```

### Pipeline Pattern
```
Source ‚Üí Extract ‚Üí [Staging] ‚Üí Transform ‚Üí [Quality Check] ‚Üí Load ‚Üí Monitor
           ‚Üì                      ‚Üì              ‚Üì            ‚Üì
       Logs CDC            Validation     Great Expect.   Alertes
```

### Formats de Donn√©es
```
CSV  ‚Üí √âchange simple, petit volume
JSON ‚Üí APIs, semi-structur√©
Parquet ‚Üí Analytics, colonnes, compression
Avro ‚Üí Streaming, sch√©ma √©volutif
```

---

## Score et Auto-√©valuation

| Score | Niveau | Recommandation |
|-------|--------|----------------|
| 23-25 | Expert | Pr√™t pour l'examen |
| 18-22 | Avanc√© | R√©viser les points faibles |
| 13-17 | Interm√©diaire | Revoir le document complet |
| < 13 | D√©butant | √âtude approfondie n√©cessaire |

---

*Test pr√©par√© pour l'examen Data Analyst - UniBank Haiti*
*Th√®me: Data Engineering - Les fondations de l'analyse*
